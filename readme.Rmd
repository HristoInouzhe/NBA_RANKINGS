---
title: "readme"
output: html_document
---

# Elastic NBA Team Rankings

## Introduction

After the 2016 presidential election, most people would probably agree that the world does not need another statistical prediction model. The election models misled us and added to the element of shock and surprise -- on both sides of the political spectrum.

Does this mean that we should we turn our backs on forecasting models? No, we just need to revise our expectations. George Box once reminded us that statistical models are, at best, useful *approximations* of the real world. With the recent hype around data science and money balling this point is often forgotten. 

So what *can* we expect from a statistical model? A good statistical should be underpinned by a solid mental model of how the world works. Moreover, we can count of models to combine a multitude of inputs with observed trends and correlations to produce a single benchmark for the future. This helps is interpret what's happening

If the inputs are bad -- such as incorrect polls -- the model is wrong. If the unthinkable happens, the model wouldn't have predicted it unless unthinkable assumptions were made.

Now, what does this have to do with the NBA? Basketball is a perfect example of a structured world with lots of information about what is happening today and lots of uncertainty around the future. If a team looks great on paper and is winning games, it'll likely do well in the future. But team chemistry, injuries, coaching changes, trades etc. can curtial success very quickly. Thus, any model-based prediction can only be viewed simply a benchmark for future success. 

This post discusses a new approach to predicting the outcome of NBA games. I've been sharing insigts from this model in prior posts about the Golden State Warriors and thohght it was time to share some broader results. I have no doubt the modes is wrong, but hopefully it's useful.


# How it Works

The Elastic NBA Team Rankings is a dynamic ranking algorithm that is purely based on statistical modeling techniques commonly used across most industries. No qualitative data or judgment is used to decide the ranks; the only human judgment applied is the underlying mental model behind the algorithm.

At a high level, the model depends on three overall factors:

* Previous performance
* The team "DNA" as measured by the roster composition of "player archetypes"
* Circumstances -- e.g., traveling, rest days, home court advantage

## More Detailed Description
The model is based on three-step procedure:

1. Group players into 25 *archetypes* using (k-means) clustering based on box-score stats. The archetype definitions were created using data prior to the 2016-2017 season; players are matched to the cluster based on their historical performance leading up to a given game. Note that from a technical perspective, the goal of the clustering algorithm is to maximize similarity of players (in terms of offensive and defensive stats) *within* clusters, while minimizing differences *between* clusters.
2. For every future game of the 2016-2017 season, predict the winner of a given game, based on the team archetypes, home team advantage, rest days, miles traveled, as well as in-season previous matchup outcomes and win-percentages.
3. Rank teams based on the predicted future win rate.

Now, why are the rankings produced by this model called "Elastic NBA Rankings?" There are two reaons for this. First, the model needed a name and "data driven" is simply too cliche. Second, the regularization technique used to fit the logistic regression model is a special case of the [Elastic Net](https://en.wikipedia.org/wiki/Elastic_net_regularization) -- one of the most robust regression techniques.

### Some Notes on the Model Used in Step 2
The model used to predict the winner of a given game is a statistical model that is estimated based on the most recent 365 "game days." Hence, the relative weights of the various drivers -- for example, the importance of different archetypes versus win percentages -- are purely based on the relationship detected from the data. For the stats-minded readers, the model is a logistic regression with an L1 penalty (lasso) to reduce the chance of overfitting (this worked best in back-testing).

The model is re-estimated every single day and contains the following variables:

*  Surplus/deficit of minutes allocated to the different archetypes. For example, if a team's lineup has more players on the court of archetype 1 than its opponent, it'll have a surplus of minutes allocated to that archetype, and vice versa for deficits. These variables basically compare the "DNAs" of the rosters.
* Short term and long term trailing winning percentages. Short term is 30 days and long term is 60 days. The winning percentages are interacted with time, which results in the model assigning less importance to win-streaks early in the season.
* Rolling overall relative point differential against opponents for the previous 90 days. This measures consistency and relative win-margin. For example, it's theoretically possible to have a winning record and also have an overall point deficit if the team suffered some big losses. Conversely, a team can have a convincing winning record but only win games by thin margins.
* Previous matchup outcomes. For example, let's say Golden State is playing the Clippers and the two teams have played eachother earlier in the season. This variable captures the outcomes of those two games. A team is more likely to win if it beat its opponent in the past.
* Distances traveled prior to the game. Traveling the day before games usually translates into weaker performances, holding everything else constant.
* Rest days prior to games. Obviously, more rest of beneficial during the long NBA season.
* Home team advantage.

### How Are Players Assigned to Archetypes?

The outcome of the k-means clustering routine used create the archetypes is a set of *centroids*. Players are assigned to archetypes by matching their offensive and defensive box score statistics to closest centroids using a simple Euclidian distance. A decay funtion was applied such that more recent games receive a larger weight. On top of that, games played in the previous season are discounted by a factor of 4.

### Predicting Allocation of Minutes for Future Games

In order to calculate the deficit and surplus variables referenced above, it's necessary to predict which players will be playing in a given game and how many minutes each player will play. Currently, a recency/frequency approach is used to guess who will suit up for a given team. Basically, the 13 players that are played most frequently and recently for a given team will ve added to the roster.

In terms of predicting the number of minutes each person will play, a 90-day trailing average is used (excluding the offseason). Games played during the prior season are discounted by a factor of 4.

### Calling the Games
The current implementation uses the esitmated probabilities from the regularized logistic regression model to call the games. If the estimated probability of a given team winning exceeds 50%, then that team is declared the winner. 

I've also been playing around with a simulation approach where each games is played, say, 1000 times -- varying the distribution of minutes across archetypes in each iteration. For the season rankings, this did not alter the overall conclusion, although it did provide a measure of the uncertainties around the predictions. 

For simulation playoffs I have been using the simulation approach. This will be covered in another post.


# Model Rankings for the 2016-2017 Season

All model rankings and results are stored in [this github repo](https://github.com/klarsen1/NBA_RANKINGS). The code below shows how to extract the current rankings. For comparison, I'm also pulling in the [FiveThirtyEight predictions](http://projects.fivethirtyeight.com/2017-nba-predictions/) and the [ESPN Power Rankings](http://www.espn.com/nba/powerrankings) (both from 11/13/2016).

```{r, echo=TRUE, message=FALSE, warning=FALSE}
library(tidyr)
library(dplyr)
library(knitr)

f1 <- "https://raw.githubusercontent.com/klarsen1/NBA_RANKINGS/master/rawdata/external_rankings.csv"
f2 <- "https://raw.githubusercontent.com/klarsen1/NBA_RANKINGS/master/rankings/rankings_2016-11-13.csv"

popular_rankings <- read.csv(f1)

all_rankings <- read.csv(f2) %>%
  inner_join(popular_rankings, by="team") %>%
  mutate(blackbox=min_rank(-pred_win_rate)) %>%
  select(team, conference, division, blackbox, ESPN, FiveThirtyEight) %>%
  arrange(blackbox)
  
kable(all_rankings)
```

The table shows that the Elastic Rankings seem to agree with the popular ratings on the "tail teams." For example, all rankings agree that Golden State, the Clippers and San Antonio will have strong seasons, and that Philadelphia and Brooklyn are going to end up at the bottom.

But what about Atlanta? The Elastic Rankings places Atlanta much higher on the list than both FiveThirtyEight and the ESPN Power Rankings. To understand why we can compare underlying parts of the model across teams and see where Atlanta stands out.

```{r, echo=TRUE, message=FALSE, warning=FALSE}
library(tidyr)
library(dplyr)
library(knitr)

f <- 
  "https://raw.githubusercontent.com/klarsen1/NBA_RANKINGS/master/modeldetails/score_decomp_2016-11-13.csv"

center <- function(x){return(x-mean(x))}
decomps <- read.csv(f, stringsAsFactors = FALSE) %>% select(-DATE) %>% 
  mutate(roster=rowSums(.[1:26]))

  mutate_each(funs(center), which(sapply(., is.numeric))) %>%
  replace(is.na(.), 0) %>%
  filter(selected_team=="Atlanta") %>%
  summarise_each(funs(mean), which(sapply(., is.numeric)))


```



## Decoding the Results
